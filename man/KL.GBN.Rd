% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/KL.R
\name{KL.GBN}
\alias{KL.GBN}
\title{KL Divergence for \code{GBN}}
\usage{
\method{KL}{GBN}(x, where, entry, delta, ...)
}
\arguments{
\item{x}{object of class \code{GBN}.}

\item{where}{character string: either \code{mean} or \code{covariance} for variations of the mean vector and covariance matrix respectively.}

\item{entry}{if \code{where == "mean"}, \code{entry} is the index of the entry of the mean vector to vary. If \code{where == "covariance"}, entry is a vector of length 2 indicating the entry of the covariance matrix to vary.}

\item{delta}{variation parameter that acts additively.}

\item{...}{additional parameters to be added to the plot.}
}
\description{
\code{KL.GBN} returns the Kullback-Leibler (KL) divergence between a Gaussian Bayesian network and its update after an additive parameter variation.
}
\references{
Gómez-Villegas, M. A., Maín, P., & Susi, R. (2007). Sensitivity analysis in Gaussian Bayesian networks using a divergence measure. Communications in Statistics—Theory and Methods, 36(3), 523-539.

Gómez-Villegas, M. A., Main, P., & Susi, R. (2013). The effect of block parameter perturbations in Gaussian Bayesian networks: Sensitivity and robustness. Information Sciences, 222, 439-458.
}
\seealso{
\code{\link{KL.CI}}, \code{\link{Fro.CI}}, \code{\link{Fro.GBN}}
}
