---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# bnmonitor

`bnmonitor` is a package for sensitivity analysis and robustness in Bayesian networks.

## Installation 

The package `bnmonitor` can be installed from GitHub using the command 
```{r eval = FALSE}
# install.packages("devtools")
devtools::install_github("manueleleonelli/bnmonitor")
```
and loaded in R with
```{r}
library(bnmonitor)
```
Note that `bnmonitor` requires the package `gRain` which, while on CRAN, depends on packages that are on Bioconductor both directly and through the `gRbase` package, which depends on `RBGL`:
```{r eval =F}
BiocManager::install()
BiocManager::install(c("graph", "Rgraphviz", "RBGL"))
install.packages("gRain")
```

## Overview

The prequential diagnostics examine the forecasts that flow from a model in sequence.
Each monitor given below indicates the probability of a particular observation based on the previous observations and the model structure. 
In the prequential mindset, we compute a probability of each subsequent observation based on all previous data points. 
These observations are then scored, and in this package we use the logarithmic score function.
The observations are then standardized to give a z-score statistic. 
Following the recommendation of Cowell (2007), scores indicate a poor fit where |z| > 1.96 

We demonstrate the efficacy of the prequential monitors with the Asia data set from the bnlearn package. Details of the variables (nodes) can be found in the documentation for bnlearn.

``` {r}
library(bnlearn)
library(bnmonitor)
data(asia)
summary(asia)
```

Lauritzen defines two candidate models we replicate below.
``` {r out.width="50%"}
asia_bn <- bnlearn::model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
bnlearn::graphviz.plot(asia_bn) 
asia_bn_alt <-  bnlearn::model2network("[A][S][T|A][L|S][B|S][E|T:L][X|E][D|B:E:S]")
bnlearn::graphviz.plot(asia_bn_alt) 
```

### Global monitor

The global monitor is equivalent to the Bayes Factor and offers a good assessment of the model as a whole. 
It is primarily useful for differentiating between candidate models as the global monitor pinpoints the nodes with different contributions for two candidate models.
We can assess the log likelihood contribution from each of the nodes and determine which nodes contribute the most to the global monitor.
global.monitor.graph provides a quick visual for the entire network. 
The darker the color, the more substantial the contribution of that node to the log likelihood.

The global monitor assesses the probability of observing the data given the model and as such does not particularly fit the prequential view. 
However, it is a quick and useful first diagnostic assessment.

``` {r}
global_monitor(asia_bn, asia, alpha = 3, FALSE)
global_monitor(asia_bn_alt, asia, alpha = 3, FALSE)
```

In the alternative model, Dysnopea contributes more to the log-likelihood. 


### Node monitor 

We compute two variants of node monitors. The marginal node monitor computes the probability of the ith observation in the data set in turn after passing the evidence of the i-1 cases in the data set.
The conditional node monitor computes the probability of the ith observation in the data set after passing evidence of the i-1 cases in the data set, and the evidence for all nodes in the ith case except the node of interest.

As a quick survey of the nodes, the node_monitor command computes the marginal and conditional monitors for the final observation in the data set. 

``` {r echo=TRUE}
node_monitor(asia_bn, asia, FALSE)
```
The scores indicate a poor fit of the probability distributions specified for the Smoking, Bronchitis, and Dysnopea nodes.
The modeller may decide which of these distributions is of the most interest. 
For the purposes of this vignette, we assume that we would like to check the forecasts for Dysnopea. 

The prequential monitor given by seq_marg_monitor give us a closer look at which particular forecasts in the data set might cause this difference.
We examine the sequential monitor for both candidate models.

``` {r,echo=TRUE, result='hide',warning=FALSE,out.width="50%"}
seq_marg_monitor(asia_bn, asia[1:200,], "D", TRUE)$plot
seq_marg_monitor(asia_bn_alt, asia[1:200,], "D", TRUE)$plot
```

Both sequential marginal node monitors fall within the recommendation of |z| > 1.96, indicating that both models are appropriate. 
However, for later forecasts in the data set, Dysnopea tends to predict higher than average observations. 
The forecasts flowing from the alternative model are more accurate.


### Parent Child monitor 

The parent child monitor computes the predictives for each subsequent observation given a particular set of parents.
Whereas the node monitors use all observations, the parent child monitors only use the observations with the specified parents.
This offers an important check for settings of parents for which the model forecasts are not appropriate.

``` {r echo=TRUE,warning=FALSE, out.width = "50%"}
seq_pa_ch_monitor(asia_bn_alt, asia, "D", "B", "yes", 3, TRUE)$plot
seq_pa_ch_monitor(asia_bn, asia, "D", "B", "no", 3, TRUE)$plot
```

These plots show that for our model under consideration, Dysnopea is adequately modeled for patients who have Bronchitis, as the z-scores largely fall in the recommended range. 
we might suggest examining the set of observations around index 1000 as those have a larger than expected z-score.
However, for patients who do not have bronchitis, we see negative z-scores that indicate that the model maybe underestimating Dysnopea in patients who do not have bronchitis.

### Influential observations

The last diagnostic monitor in this package focuses on specific observations.
We compute the log-log likelihood of a model omitting each subsequent data point, and plot the ratio of this log-likelihood over the one with the whole model.
This Bayes factor indicates which observations are particularly surprising. 
Any patterns within this observation might indicate a subset of data for which the model is a particularly poor fit. 

For the Asia data set, we suggest examining observations with a leave one out score higher than 6.
```{r echo=TRUE, warning=FALSE}
influential_obs(asia_bn, asia[1:200,], 3, TRUE)$plot
```


#### Discrete Bayesian Networks
The functionalities of `bnmonitor` for sensitivity analysis in Discrete Bayesian Networks are illustrated using the `fire_alarm` network bundled within the package representing a simple alarm system for fire accidents. 

```{r out.width="50%"}
library(bnlearn)
graphviz.plot(fire_alarm)
```

The network consists of 6 nodes (`Fire`, `Smoke`, `Tampering`, `Alarm`, `Leaving` and `Report`) of either two or three levels. The conditional probability tables can be investigated by typing `fire_alarm`.


```{r}
fire_alarm
```

The aim of a sensitivity analysis is to assess how parameter changes affect output probabilities of interest. When one parameter changes than others need to covary in order to respect the sum to one condition of probabilities. In the binary case this is obvious, since the covaried probability is equal to one minus the varied one. However, when there are more than two probabilities there are multiple covariation schemes that can be applied, including `proportional`, `uniform` and `order-preserving`. Suppose we vary the probability that `Alarm = FALSE` given that `Fire = TRUE` and `Tampering = FALSE` from 0.01 to 0.1. The resulting probabilities from different covariation schemes can be computed as follows.

```{r}
proportional_covar(fire_alarm,"Alarm", "FALSE", c("TRUE","FALSE"), 0.1)$Alarm$prob
uniform_covar(fire_alarm,"Alarm", "FALSE", c("TRUE","FALSE"), 0.1)$Alarm$prob
orderp_covar(fire_alarm,"Alarm", "FALSE", c("TRUE","FALSE"), 0.1)$Alarm$prob
```

Suppose we are interested in the probability that `Report = TRUE` and how this varies with respect to the probability that `Alarm = FALSE` conditional on `Fire = TRUE` and `Tampering = FALSE`. This functional relationship is reported by the sensitivity function which can be computed and plotted using the function `sensitivity`.

```{r out.width="35%"}
sensitivity(fire_alarm, "Report", "TRUE" , node = "Alarm" , value_node = "FALSE", value_parents = c("TRUE","FALSE"), new_value = "all", covariation = "all")
```

For all covariation schemes as the conditional probability of `Alarm = FALSE` increases, the probability of `Report = TRUE` decreases. The uniform covariation scheme is reported in red, the proportional in green and the order-preserving in blue.

The function sensitivity also allows for conditional probabilities of interest, not only marginals, which can be set by the options `evidence_nodes` and `evidence_states`.

For such changes in the probability distribution of a Bayesian networks, we can compute the distance between the original and the varied networks using the Chan-Darwiche distance (`CD`) and the Kullback-Leibler divergence (`KL`). As an example we use `CD` for `Alarm = MALFUNCTIOn` conditional on `Fire = TRUE` and `Tampering = FALSE`.

```{r out.width="35%"}
CD(fire_alarm, node = "Alarm" , value_node = "MALFUNCTION", value_parents = c("TRUE","FALSE"), new_value = "all", covariation = "all")
```

As expected proportional covariation has the smallest CD distance of all covariation schemes.

Another task in sensitivity analysis is to identify parameter changes that meet a specific constraint. As an example suppose we want the probability `Report = TRUE` to be equal to 0.4. Which parameters could we change to obtain such a probability? The answer can be obtained using the function `sensquery`.

```{r}
sensquery(fire_alarm,"Report","TRUE", 0.4)
```

In `sensquery` covarying parameters are always changed according to a proportional scheme. The output reports the parameters that could be changed, which value they need to be changed to, and the CD distance between the original and the varied network.



## Gaussian Bayesian Networks
The functionalities of `bnmonitor` for sensitivity analysis in Gaussian Bayesian networks are illustrated next using the `mathmarks` dataset bundled within the package. 
```{r}
data(mathmarks)
head(mathmarks)
```
The data includes the grades (out of 100) of students in five maths exams: mechanics, vectors, algebra, analysis and statistics.

The structure of a Bayesian network for this data is first learnt using the package `bnlearn` and the maximum likelihood estimate of its parameters is computed and stored in `bnfit`.
```{r out.width="50%"}
library(bnlearn)
bn <- hc(mathmarks)
plot(bn)
bnfit <-bn.fit(bn,mathmarks)
```

To start the sensitivity analysis for the parameters of the learnt Bayesian network, one first need to transform `bnfit` to objects of class `GBN` (for standard sensitivity analysis) and `CI` (for model-preserving sensitivity). This can be done using the functions `bn2gbn` and `bn2ci` respectively.
```{r}
gbn <- bn2gbn(bnfit)
ci <-  bn2ci(bnfit)
c(class(gbn),class(ci))
```


#### Perturbation of the mean vector

A varied GBN after a perturbation of an entry of the mean vector can be obtained with the function `mean_var`, which can only be applied to an object of class `GBN`. Below, we vary the fifth entry of the mean vector (statistics) by an additive factor 10.

```{r}
rbind(gbn$order,round(t(gbn$mean),2),round(t(mean_var(gbn,entry = 5, delta = 10)$mean),2))
```

The overall effect of such variations can be assessed in terms of dissimilarity measures: the Kullback-Leibler divergence (`KL`) and Jeffrey's divergence (`Jeffreys`). For instance, let's see what's the effect of variations in the mean of the statistics exam.
```{r out.width="35%", warning = FALSE}
mean_var5 <- KL(gbn, "mean", entry=5, delta = seq(-10,10,0.1))
mean_var5$plot
```


More interestingly, one can check the different effect of variations of different paramenters.
```{r echo = FALSE, out.width="45%", warning = FALSE}
library(ggplot2)
par(mfrow = c(1,2))
mean_var1 <- KL(gbn, "mean", entry=1, delta = seq(-10,10,0.1))$KL
mean_var2 <- KL(gbn, "mean", entry=2, delta = seq(-10,10,0.1))$KL
mean_var3 <- KL(gbn, "mean", entry=3, delta = seq(-10,10,0.1))$KL
mean_var4 <- KL(gbn, "mean", entry=4, delta = seq(-10,10,0.1))$KL
mean_var5 <- KL(gbn, "mean", entry=5, delta = seq(-10,10,0.1))$KL
out <- data.frame(delta = rep(seq(-10,10,0.1),5), KL = c(mean_var1[,2],mean_var2[,2],mean_var3[,2],mean_var4[,2],mean_var5[,2]), Entry = c(rep("mechanics",length(seq(-10,10,0.1))),rep("vectors",length(seq(-10,10,0.1))),rep("algebra",length(seq(-10,10,0.1))),rep("analysis",length(seq(-10,10,0.1))),rep("statistics",length(seq(-10,10,0.1)))))
mean_var1J <- Jeffreys(gbn, "mean", entry=1, delta = seq(-10,10,0.1))$Jeffreys
mean_var2J <- Jeffreys(gbn, "mean", entry=2, delta = seq(-10,10,0.1))$Jeffreys
mean_var3J <- Jeffreys(gbn, "mean", entry=3, delta = seq(-10,10,0.1))$Jeffreys
mean_var4J <- Jeffreys(gbn, "mean", entry=4, delta = seq(-10,10,0.1))$Jeffreys
mean_var5J <- Jeffreys(gbn, "mean", entry=5, delta = seq(-10,10,0.1))$Jeffreys
out1 <- data.frame(delta = rep(seq(-10,10,0.1),5), Jeffreys = c(mean_var1J[,2],mean_var2J[,2],mean_var3J[,2],mean_var4J[,2],mean_var5J[,2]), Entry = c(rep("mechanics",length(seq(-10,10,0.1))),rep("vectors",length(seq(-10,10,0.1))),rep("algebra",length(seq(-10,10,0.1))),rep("analysis",length(seq(-10,10,0.1))),rep("statistics",length(seq(-10,10,0.1)))))
ggplot(out) + geom_line(aes(x=delta,y=KL,color = Entry)) + theme_minimal()
ggplot(out1) + geom_line(aes(x=delta,y=Jeffreys,color = Entry)) + theme_minimal()
```



Therefore, misspecifications of the mean of the algebra exam would have the biggest effect on the distribution of the Gaussian Bayesian network.

#### Perturbation of the covariance matrix

Care must be taken when performing perturbations of the covariance matrix, for two reasons: (1) the perturbed matrix may not be positive semidefinite; (2) the perturbed matrix may not respect the conditional indepedences of the underlying Bayesian network. 

Suppose we are interested in assessing the effect of varying the covariance between `Statistics` and `Vectors` corresponding to the entry (2,5) of the covariance matrix below.

```{r}
gbn$order
gbn$covariance
```

A standard perturbated covariance matrix can be constructed with the `covariance_var` function. Suppose we want to increase the covariance between `Statistics` and `Vectors` by a factor of 10. 
```{r}
d <- 10
covariance_var(gbn, c(2,5), d)$covariance
```

The above perturbation made the original network structure not valid for the new covariation matrix. In order to ensure that the perturbed covariance is still valid for the underlying network structure, we can use model-preserving methods. These apply multiplicatively and not additively as standard methods, but we apply the same change in the covariance via the perturbation `delta` defined below. We can construct various covarioation matrices using the following commands:

```{r}
delta <- (d + gbn$covariance[2,5])/gbn$covariance[2,5]
total_covar_matrix(ci,c(2,5), delta)
partial_covar_matrix(ci,c(2,5),delta)
row_covar_matrix(ci,c(2,5),delta)
col_covar_matrix(ci,c(2,5),delta)
```

For any of the four available methods (`total`, `partial`, `row` and `column`) the perturbed covariance matrix can be calculated with the function `model_pres_cov`. For instance in the case of a partial covariation:
```{r}
model_pres_cov(ci,"partial",c(2,5),delta)$covariance
```

Having constructed various covariation matrices, we can assess how far apart the original and the perturbed distributions are for various covariations methods. Available dissimilarity measures are Frobenius norm (`Fro`), Kullback-Leibler divergence (`KL`) and Jeffrey's divergence (`Jeffreys`). The code below computes the Jefrrey's divergence for multiple variation values `d` for both the standard approach and the model-preserving one with a partial covariation matrix.

```{r}
d <- seq(-10,10,0.1)
delta <- (d+gbn$covariance[2,5])/gbn$covariance[2,5]
standard <- Jeffreys(gbn,"covariance", c(2,5), d)$Jeffreys
partial <- Jeffreys(ci,"partial",c(2,5),delta)$Jeffreys
```


Let's compare the methods.

```{r, echo = FALSE, out.width="45%"}
total <- Jeffreys(ci,"total",c(2,5),delta)$Jeffreys
row <- Jeffreys(ci,"row",c(2,5),delta)$Jeffreys
column <- Jeffreys(ci,"column",c(2,5),delta)$Jeffreys
out <- data.frame(d = c(d,d,d,d,d), method = c(rep("standard",length(d)),rep("partial",length(d)),rep("total",length(d)),rep("row",length(d)),rep("column",length(d))), Jeffreys = c(standard$Jeffreys,partial$Jeffreys,total$Jeffreys,row$Jeffreys,column$Jeffreys))
ggplot(out) + geom_line(aes(x=d,y=Jeffreys,color = method)) 
```

The standard approach has the smallest Jeffreys divergence (this expected, although not guaranteed, since it changes the smallest number of parameters). Column-based and partial covariation have a similar Jeffreys divergence and not too far from the one of the standard method.

As for the mean, we can check which entry of the covariance matrix has the biggest impact if varied. For simplicity here we pick the standard method only.

```{r,echo=FALSE, out.width="45%"}
d <- seq(-5,5,0.1)
standard11 <- Jeffreys(gbn,"covariance", c(1,1), d)$Jeffreys
standard12 <- Jeffreys(gbn,"covariance", c(1,2), d)$Jeffreys
standard13 <- Jeffreys(gbn,"covariance", c(1,3), d)$Jeffreys
standard14 <- Jeffreys(gbn,"covariance", c(1,4), d)$Jeffreys
standard15 <- Jeffreys(gbn,"covariance", c(1,5), d)$Jeffreys
standard22 <- Jeffreys(gbn,"covariance", c(2,2), d)$Jeffreys
standard23 <- Jeffreys(gbn,"covariance", c(2,3), d)$Jeffreys
standard24 <- Jeffreys(gbn,"covariance", c(2,4), d)$Jeffreys
standard25 <- Jeffreys(gbn,"covariance", c(2,5), d)$Jeffreys
standard33 <- Jeffreys(gbn,"covariance", c(3,3), d)$Jeffreys
standard34 <- Jeffreys(gbn,"covariance", c(3,4), d)$Jeffreys
standard35 <- Jeffreys(gbn,"covariance", c(3,5), d)$Jeffreys
standard44 <- Jeffreys(gbn,"covariance", c(4,4), d)$Jeffreys
standard45 <- Jeffreys(gbn,"covariance", c(4,5), d)$Jeffreys
standard55 <- Jeffreys(gbn,"covariance", c(5,5), d)$Jeffreys
dtot <- rep(d,15)
l <- length(d)
entry <- c(rep("mechanics",l),rep("mechanics/vectors",l),rep("mechanics/algebra",l),rep("mechanics/analysis",l),rep("mechanics/statistics",l),rep("vectors",l),rep("vectors/algebra",l),rep("vectors/analysis",l),rep("vectors/statistics",l), rep("algebra",l), rep("algebra/analysis",l),rep("algebra/statistics",l),rep("analysis",l),rep("analysis/statistics",l),rep("statistics",l))
Jef <- c(standard11$Jeffreys,standard12$Jeffreys,standard13$Jeffreys,standard14$Jeffreys,standard15$Jeffreys,standard22$Jeffreys,standard23$Jeffreys,standard24$Jeffreys,standard25$Jeffreys,standard33$Jeffreys,standard34$Jeffreys,standard35$Jeffreys,standard44$Jeffreys,standard45$Jeffreys,standard55$Jeffreys)
out <- data.frame(d = dtot, entry = entry, Jeffreys = Jef)
ggplot(out) + geom_line(aes(x = d, y = Jeffreys, color=entry))
```

From the above plot we can notice that the less robust entries of the covariance matrix are the variance of `algebra`, the covariance between `algebra` and `analysis`, and the covariance between `algebra` and `vectors`.

Another method to quickly have an overview of the effect of all parameters is `KL_bounds` which creates a table with upper bounds to the Kullback-Leibler divergence for all entries of the covariance matrix and all covariation methods.

```{r}
KL_bounds(ci, 1.2)
```

By looking at the standard method column, we have the confirmation that the 11th entry, corresponding to `algebra/analysis`, is the most critical for the robustness of the network.





